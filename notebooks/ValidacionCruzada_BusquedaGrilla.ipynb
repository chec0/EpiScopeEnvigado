{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9de0d2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  √ó Getting requirements to build wheel did not run successfully.\n",
      "  ‚îÇ exit code: 1\n",
      "  ‚ï∞‚îÄ> [15 lines of output]\n",
      "      The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "      rather than 'sklearn' for pip commands.\n",
      "      \n",
      "      Here is how to fix this error in the main use cases:\n",
      "      - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "      - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "        (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "      - if the 'sklearn' package is used by one of your dependencies,\n",
      "        it would be great if you take some time to track which package uses\n",
      "        'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "      - as a last resort, set the environment variable\n",
      "        SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "      \n",
      "      More information is available at\n",
      "      https://github.com/scikit-learn/sklearn-pypi-package\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "√ó Getting requirements to build wheel did not run successfully.\n",
      "‚îÇ exit code: 1\n",
      "‚ï∞‚îÄ> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e879db92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer , make_blobs, load_iris\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import sys\n",
    "sys.path.append('../episcopeenvigado')\n",
    "\n",
    "import episcopeenvigado.dataset as ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ae22e1",
   "metadata": {},
   "source": [
    "# B√∫squeda en Rejilla y Validaci√≥n Cruzada\n",
    "\n",
    "La validaci√≥n cruzada es un m√©todo para evaluar el rendimiento.\n",
    "\n",
    "La b√∫squeda en rejilla, es un m√©todo para ajustar los par√°metros en modelos supervisados ‚Äã‚Äãy mejorar el rendimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95efcd5f",
   "metadata": {},
   "source": [
    "## Validaci√≥n Cruzada \n",
    "\n",
    "Es un m√©todo estad√≠stico para evaluar la capacidad de generalizaci√≥n: Los datos se dividen en pliegues (k-fold) y se entrenan m√∫ltiples modelos en cada pliegue. \n",
    "\n",
    "El primer modelo se entrena utilizando el pliegue 1 como conjunto de prueba, y los pliegues  2-5 como conjunto de entrenamiento. Posteriormente, se construye otro modelo con el pliegue 2 como conjunto de prueba y los datos de los pliegues 1, 3, 4 y 5 como conjunto de entrenamiento, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29d9b2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-30 20:47:40.623\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mepiscopeenvigado.dataset\u001b[0m:\u001b[36mobtener_dataset_completo\u001b[0m:\u001b[36m109\u001b[0m - \u001b[1müìã Se encontraron 6 tablas en la base de datos.\u001b[0m\n",
      "\u001b[32m2025-10-30 20:47:40.637\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mepiscopeenvigado.dataset\u001b[0m:\u001b[36mobtener_dataset_completo\u001b[0m:\u001b[36m118\u001b[0m - \u001b[32m\u001b[1m‚úÖ Tabla 'dim_causa_ext' cargada correctamente (14 filas).\u001b[0m\n",
      "\u001b[32m2025-10-30 20:47:40.643\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mepiscopeenvigado.dataset\u001b[0m:\u001b[36mobtener_dataset_completo\u001b[0m:\u001b[36m118\u001b[0m - \u001b[32m\u001b[1m‚úÖ Tabla 'dim_departamento' cargada correctamente (34 filas).\u001b[0m\n",
      "\u001b[32m2025-10-30 20:47:40.651\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mepiscopeenvigado.dataset\u001b[0m:\u001b[36mobtener_dataset_completo\u001b[0m:\u001b[36m118\u001b[0m - \u001b[32m\u001b[1m‚úÖ Tabla 'dim_estado_salida' cargada correctamente (2 filas).\u001b[0m\n",
      "\u001b[32m2025-10-30 20:47:40.675\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mepiscopeenvigado.dataset\u001b[0m:\u001b[36mobtener_dataset_completo\u001b[0m:\u001b[36m118\u001b[0m - \u001b[32m\u001b[1m‚úÖ Tabla 'dim_municipio' cargada correctamente (1124 filas).\u001b[0m\n",
      "\u001b[32m2025-10-30 20:47:40.683\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mepiscopeenvigado.dataset\u001b[0m:\u001b[36mobtener_dataset_completo\u001b[0m:\u001b[36m118\u001b[0m - \u001b[32m\u001b[1m‚úÖ Tabla 'dim_via_ingreso' cargada correctamente (9 filas).\u001b[0m\n",
      "\u001b[32m2025-10-30 20:47:41.905\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mepiscopeenvigado.dataset\u001b[0m:\u001b[36mobtener_dataset_completo\u001b[0m:\u001b[36m118\u001b[0m - \u001b[32m\u001b[1m‚úÖ Tabla 'fact_atenciones' cargada correctamente (45303 filas).\u001b[0m\n",
      "\u001b[32m2025-10-30 20:47:41.906\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mepiscopeenvigado.dataset\u001b[0m:\u001b[36mobtener_dataset_completo\u001b[0m:\u001b[36m124\u001b[0m - \u001b[1m‚úÖ Dataset completo cargado (6 tablas exitosas).\u001b[0m\n",
      "   fact_id      Cod_IPS        ID Fecha_Ingreso Fecha_Egreso  Duracion_Dias  \\\n",
      "0        1  52660212605  PAC00001    2023-09-04   2023-09-14             10   \n",
      "1        2  52660212605  PAC00002    2023-02-05   2023-02-28             23   \n",
      "2        3  52660212605  PAC00003    2023-09-30   2023-09-30              0   \n",
      "3        4  52660212605  PAC00004    2023-08-31   2023-09-29             29   \n",
      "4        5  52660212605  PAC00005    2023-08-26   2023-09-01              6   \n",
      "\n",
      "   via_ingreso_id  estado_salida_id  municipio_id  causa_ext_id  ...  \\\n",
      "0               3                 1           114            12  ...   \n",
      "1               2                 1           114            12  ...   \n",
      "2               3                 1             1            12  ...   \n",
      "3               3                 1             1            12  ...   \n",
      "4               3                 1             1             2  ...   \n",
      "\n",
      "   DEPARTAMENTO  MUNICIPIO_DANE DIAGNOSTICO INGRESO  Cod_Dx_Ppal_Egreso  \\\n",
      "0            05           05266                R42X                F28X   \n",
      "1            05           05266                R51X                A971   \n",
      "2            05           05001                R688                B159   \n",
      "3            05           05001                B24X                G042   \n",
      "4            05           05001                T140                S822   \n",
      "\n",
      "   DIAG EGRESO REL 1  DIAG EGRESO REL 2  DIAG EGRESO REL 3  DIAG COMPLICACION  \\\n",
      "0               None               None               None               None   \n",
      "1               None               None               None               None   \n",
      "2               None               None               None               None   \n",
      "3               None               None               None               None   \n",
      "4               None               None               None               None   \n",
      "\n",
      "  DIAG MUERTE   A√ëO  \n",
      "0        None  2023  \n",
      "1        None  2023  \n",
      "2        None  2023  \n",
      "3        None  2023  \n",
      "4        None  2023  \n",
      "\n",
      "[5 rows x 28 columns]\n",
      "Exactitud de la validaci√≥n cruzada: [0.96666667 1.         0.93333333 0.96666667 1.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Diego Eusse\\Documents\\ANALISIS DE DATOS\\Proyecto Final\\EpiScopeEnvigado\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "iris = load_iris()\n",
    "logreg = LogisticRegression()\n",
    "episcope_data = ds.obtener_dataset_completo()\n",
    "\n",
    "dim_fact = episcope_data[\"fact_atenciones\"] \n",
    "\n",
    "print(dim_fact.head())\n",
    "#Por defecto, k=3\n",
    "scores = cross_val_score(logreg, iris.data, iris.target, cv=5) \n",
    "print(\"Exactitud de la validaci√≥n cruzada: {}\".format(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdd6475",
   "metadata": {},
   "source": [
    "### Validaci√≥n Cruzada estratificada\n",
    "\n",
    "Para modelos de clasificaci√≥n, es recomendable utilizar la validaci√≥n cruzada estratificada: dividimos los datos de manera que las proporciones entre las clases sean las mismas en cada partici√≥n que en el conjunto de datos completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0901019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mglearn\n",
    "mglearn.plots.plot_stratified_cross_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c1355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Iris labels:\\n{}\".format(iris.target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2748db65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold \n",
    "kfold = KFold(n_splits=5)\n",
    "print(\"Cross-validation scores:\\n{}\".format( cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa640ce",
   "metadata": {},
   "source": [
    "Otra opci√≥n es aleatorizar los datos en lugar de estratificar los pliegues, pero debemos fijar `random_state=0`. De lo contrario, cada ejecuci√≥n de `cross_val_score` arrojar√≠a un resultado diferente, ya que se usar√≠a una divisi√≥n distinta en cada ocasi√≥n.\n",
    "\n",
    "`kfold = KFold(n_splits=3, shuffle=True, random_state=0)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737c9768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#leave-one-out: Cada fold representa una sola muestra \n",
    "# Para cada divisi√≥n, se selecciona un dato para el conjunto de prueba. (muy lento para grandes datos)\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "loo = LeaveOneOut()\n",
    "scores = cross_val_score(logreg, iris.data, iris.target, cv=loo) \n",
    "print(\"N√∫mero de iteraciones sobre cv: \", len(scores))\n",
    "print(\"Media del puntaje: {:.2f}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bcf8f3",
   "metadata": {},
   "source": [
    "### Validaci√≥n cruzada con divisi√≥n aleatoria\n",
    "\n",
    "Cada divisi√≥n toma puntos para el conjunto de entrenamiento de `train_size` y para el conjunto de prueba de `test_size`. \n",
    "Esta divisi√≥n se repite `n_iter` veces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e688682",
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_shuffle_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04cf60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# Divici√≥n del conjunto en 50% para entrenamiento y un 50% para prueba para 10 iteraciones\n",
    "shuffle_split = ShuffleSplit(test_size=.5, train_size=.5, n_splits=10) \n",
    "scores = cross_val_score(logreg, iris.data, iris.target, cv=shuffle_split) \n",
    "print(\"Extactidudes sobre la Validaci√≥n cruzada:\\n{}\".format(scores))\n",
    "print('='*30)\n",
    "print('Tambi√©n se puede usar solo una parte de los datos en cada iteraci√≥n\\nbasta con dar valores distintos en `train_size` y `test_size`\\nEste submuestreo puede ser √∫til con grandes conjuntos de datos.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8b4a05",
   "metadata": {},
   "source": [
    "### Validaci√≥n cruzada con grupos\n",
    "\n",
    "Cuando existen grupos de datos altamente relacionados, podemos usar `GroupKFold`, que toma como argumento un array que indica los grupos de datos que no deben dividirse al crear los conjuntos de entrenamiento y prueba, y no debe confundirse con la etiqueta de clase.\n",
    "\n",
    "Este caso es com√∫n en aplicaciones m√©dicas, donde se pueden tener m√∫ltiples muestras del mismo paciente, pero se busca generalizar a nuevos pacientes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbb8612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "X, y = make_blobs(n_samples=12, random_state=0)\n",
    "# las tres primeras muestras pertenecen al mismo grupo,\n",
    "# las cuatro siguientes pertenecen al mismo grupo, etc.\n",
    "groups=[0,0,0,1,1,1,1,2,2,3,3,3]\n",
    "scores = cross_val_score(estimator=logreg, X=X, y=y, groups=groups, cv=GroupKFold(n_splits=2)) \n",
    "print(\"Extactidudes sobre la Validaci√≥n cruzada:\\n{}\".format(scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1367788",
   "metadata": {},
   "source": [
    "## B√∫squeda en Rejilla\n",
    "\n",
    "Es un m√©todo para encontrar los valores de los par√°metros para un mejor modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d5dd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=0)\n",
    "\n",
    "\n",
    "for gamma in [0.001, 0.01, 0.1, 1, 10, 100]: \n",
    "        for C in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "            # Para cada combinaci√≥n de par√°metros, entrena SVC.\n",
    "            svm = SVC(gamma=gamma, C=C)\n",
    "            svm.fit(X_train, y_train)\n",
    "            # Eval√∫a SVC en el conjunto de prueba\n",
    "            score = svm.score(X_test, y_test)\n",
    "            # Almacena la puntuaci√≥n y los par√°metros para la mejor puntuaci√≥n mejor \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_parameters = {'C': C, 'gamma': gamma}\n",
    "\n",
    "print(\"Mejor exactitud: {:.2f}\".format(best_score)) \n",
    "print(\"Mejores par√°metros: {}\".format(best_parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06c3760",
   "metadata": {},
   "source": [
    "### Overfiting\n",
    "\n",
    "Aunque la precisi√≥n sobre los datos de prueba es 97%, no necesariamente la precisi√≥n se mantendr√° con nuevos datos. \n",
    "\n",
    "Dado que usamos los datos de prueba para ajustar los par√°metros, ya no podemos usarlos para evaluar la calidad del modelo. Una forma de resolver este problema es dividir los datos nuevamente, de modo que tengamos tres conjuntos: \n",
    "1. datos de entrenamiento para construir el modelo\n",
    "2. datos de validaci√≥n (o desarrollo) para seleccionar los par√°metros del modelo\n",
    "3. datos de prueba para evaluar el rendimiento de los par√°metros seleccionados. \n",
    "\n",
    "Seleccionamos los mejores par√°metros usando el conjunto de validaci√≥n y se entrena con los datos de entrenamiento y con los de validaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3802640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divisi√≥on train+validation dataset and test dataset\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(iris.data, iris.target, random_state=0)\n",
    "\n",
    "# Divisi√≥n train+validation dataset en training data y validation data  \n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_trainval, y_trainval, random_state=1)\n",
    "best_score = 0\n",
    "for gamma in [0.001, 0.01, 0.1, 1, 10, 100]: \n",
    "        for C in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "            # Para cada combinaci√≥n de par√°metros, entrena SVC.\n",
    "            svm = SVC(gamma=gamma, C=C)\n",
    "            svm.fit(X_train, y_train)\n",
    "            # Eval√∫a SVC en el conjunto de prueba \n",
    "            score = svm.score(X_valid, y_valid)\n",
    "            # Almacena la puntuaci√≥n y los par√°metros para la mejor puntuaci√≥n mejor \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_parameters = {'C': C, 'gamma': gamma}\n",
    "\n",
    "# Entreanmos el modelo con train+validation dataset y lo evaluamos con test dataset.\n",
    "svm = SVC(**best_parameters)\n",
    "svm.fit(X_trainval, y_trainval)\n",
    "test_score = svm.score(X_test, y_test)\n",
    "print(\"Mejor puntaje sobre el conjunti de validaci√≥n: {:.2f}\".format(best_score))\n",
    "print(\"Mejores parametros: \", best_parameters)\n",
    "print(\"Puntaje sobre datos de prueba con los mejores parametros: {:.2f}\".format(test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c280f81d",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba19f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=42)\n",
    "\n",
    "# Pipeline de dos pasos: \"scaler\" y \"svm\".\n",
    "pipe = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC())])\n",
    "# .fit para usar el pipeline.\n",
    "pipe.fit(X_train, y_train)\n",
    "# .score para el puntaje del modelo\n",
    "print(\"Test score: {:.2f}\".format(pipe.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3511c5d",
   "metadata": {},
   "source": [
    "## Pipelines para b√∫squeda en regillas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea403e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La sintaxis para definir una grilla de par√°metros para un Pipeline: \n",
    "# (Nombre del paso)__(nombre del par√°metro). \n",
    "param_grid = {'svm__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "                  'svm__gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, cv=5) \n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Mejor exactitud de validaci√≥n cuzada: {:.2f}\".format(grid.best_score_)) \n",
    "print(\"R^2 score: {:.2f}\".format(grid.score(X_test, y_test))) \n",
    "print(\"Mejores par√°metros: {}\".format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0146b1c2",
   "metadata": {},
   "source": [
    "## Fuga de informaci√≥n en la validaci√≥n cruzada\n",
    "\n",
    "Estimar la escala de los datos mediante el pliegue de prueba no suele tener un impacto significativo, mientras que su uso en la extracci√≥n y selecci√≥n de caracter√≠sticas puede generar diferencias sustanciales en los resultados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38fb3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conjunto de datos, donde no existe relaci√≥n entre variables\n",
    "rnd = np.random.RandomState(seed=0)\n",
    "X = rnd.normal(size=(100, 10000))\n",
    "y = rnd.normal(size=(100,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f14b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectPercentile, f_regression\n",
    "\n",
    "#SelectPercentile: seleccina la caracter√≠stica del percentil con R^2 m√°s alto\n",
    "# y entrena el modelo mediante validaci√≥n cruzada\n",
    "select = SelectPercentile(score_func=f_regression, percentile=5).fit(X, y) \n",
    "X_selected = select.transform(X)\n",
    "print(\"X_selected.shape: {}\".format(X_selected.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a110348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "print(\"Cross-validation accuracy (cv only on ridge): {:.2f}\".format(\n",
    "    np.mean(cross_val_score(Ridge(), X_selected, y, cv=5))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab03eaf5",
   "metadata": {},
   "source": [
    "El $R^2$ indica un modelo muy bueno. **Esto no puede ser correcto**, los datos son aleatorios.\n",
    "\n",
    "¬øQu√© sucedi√≥? la selecci√≥n de caracter√≠sticas eligi√≥ algunas (entre las 10 000 aleatorias), que (por azar) est√°n bien correlacionadas. \n",
    "\n",
    "Dado que ajustamos la selecci√≥n de caracter√≠sticas fuera de la validaci√≥n cruzada, pudo encontrar caracter√≠sticas correlacionadas tanto en el conjunto de entrenamiento como en el de prueba. La informaci√≥n que se filtr√≥ del conjunto de prueba fue muy informativa, lo que condujo a resultados muy poco realistas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab6a2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([(\"select\", SelectPercentile(score_func=f_regression,\n",
    "                                                 percentile=5)),(\"ridge\", Ridge())]) \n",
    "\n",
    "print(\"Cross-validation accuracy (pipeline): {:.2f}\".format(\n",
    "    np.mean(cross_val_score(pipe, X, y, cv=5))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309c1369",
   "metadata": {},
   "source": [
    "$R^2$ cercano a cero, un modelo deficiente.\n",
    "\n",
    "Con el Pipeline la selecci√≥n de caracter√≠sticas se encuentra ahora dentro del bucle de validaci√≥n cruzada. Esto significa que las caracter√≠sticas solo se pueden seleccionar utilizando los pliegues de entrenamiento de los datos, no el pliegue de prueba. \n",
    "\n",
    "Se corrige el problema de fuga de datos en la selecci√≥n de caracter√≠sticas y marca la diferencia entre concluir que un modelo funciona muy bien o concluir que no funciona en absoluto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00474d28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
